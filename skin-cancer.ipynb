{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el dataset\n",
    "Cargar el dataset que son imagenes que están dentro de carpetas separadas por test y train y cada una separada por maligno y benigno.\n",
    "\n",
    "Crearemos un dataset en el que por cada fila se incluye el path y si es benigno o maligno. Crearemos uno para el train y otro para el test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el dataframe de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4001\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Train\n",
    "def train_df(train_path):\n",
    "    classes = [] # Benignas o malignas\n",
    "    class_paths = [] # Paths de las imagenes\n",
    "    image_names = [] # Nombre de las imagenes\n",
    "\n",
    "    files = os.listdir(train_path) # Archivos\n",
    "    for file in files: \n",
    "        label_dir = os.path.join(train_path, file) # Path del directorio\n",
    "        label = os.listdir(label_dir) # Imagenes del directorio\n",
    "        for image in label:\n",
    "            image_names.append(image) \n",
    "            class_paths.append(label_dir) \n",
    "            classes.append(file) # Añade el nombre del archivo\n",
    "\n",
    "    # Series de pandas\n",
    "    class_paths = pd.Series(class_paths, name='Class Path')\n",
    "    image_names = pd.Series(image_names, name='Image name')\n",
    "    image_classes = pd.Series(classes, name='Class') \n",
    "\n",
    "    # Crea el dataframe\n",
    "    tr_df = pd.concat([class_paths, image_names, image_classes], axis=1)\n",
    "    return tr_df\n",
    "\n",
    "tr_df = train_df('./skin-cancer-dataset/train')\n",
    "print(len(tr_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el dataframe de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "def test_df(test_path):\n",
    "    classes = [] # Benignas o malignas\n",
    "    class_paths = [] # Ruta a las imagenes\n",
    "    image_names = [] # Nombre de las imagenes\n",
    "\n",
    "    files = os.listdir(test_path) # Archivos\n",
    "    for file in files: \n",
    "        label_dir = os.path.join(test_path, file) # Path del directorio\n",
    "        label = os.listdir(label_dir) # Imagenes dentro del directorio\n",
    "        for image in label:\n",
    "            image_names.append(image) \n",
    "            class_paths.append(label_dir) \n",
    "            classes.append(file) # Añade el nombre del archivo\n",
    "\n",
    "    # Series de pandas\n",
    "    class_paths = pd.Series(class_paths, name='Class Path')\n",
    "    image_names = pd.Series(image_names, name='Image name')\n",
    "    image_classes = pd.Series(classes, name='Class') \n",
    "\n",
    "    # Crea el dataframe\n",
    "    ts_df = pd.concat([class_paths, image_names, image_classes], axis=1)\n",
    "    return ts_df\n",
    "\n",
    "ts_df = test_df('./skin-cancer-dataset/test')\n",
    "print(len(ts_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar los datos entre 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Class Path Image name  Class\n",
      "0     ./skin-cancer-dataset/train/Malignant     63.jpg      1\n",
      "1     ./skin-cancer-dataset/train/Malignant    823.jpg      1\n",
      "2     ./skin-cancer-dataset/train/Malignant   1409.jpg      1\n",
      "3     ./skin-cancer-dataset/train/Malignant    189.jpg      1\n",
      "4     ./skin-cancer-dataset/train/Malignant     77.jpg      1\n",
      "...                                     ...        ...    ...\n",
      "3996     ./skin-cancer-dataset/train/Benign    190.jpg      0\n",
      "3997     ./skin-cancer-dataset/train/Benign   1404.jpg      0\n",
      "3998     ./skin-cancer-dataset/train/Benign   1410.jpg      0\n",
      "3999     ./skin-cancer-dataset/train/Benign    184.jpg      0\n",
      "4000     ./skin-cancer-dataset/train/Benign   1376.jpg      0\n",
      "\n",
      "[4001 rows x 3 columns]                                 Class Path Image name  Class\n",
      "0     ./skin-cancer-dataset/test/Malignant   6400.jpg      1\n",
      "1     ./skin-cancer-dataset/test/Malignant   6366.jpg      1\n",
      "2     ./skin-cancer-dataset/test/Malignant   6372.jpg      1\n",
      "3     ./skin-cancer-dataset/test/Malignant   6414.jpg      1\n",
      "4     ./skin-cancer-dataset/test/Malignant   5653.jpg      1\n",
      "...                                    ...        ...    ...\n",
      "1995     ./skin-cancer-dataset/test/Benign   6343.jpg      0\n",
      "1996     ./skin-cancer-dataset/test/Benign   6425.jpg      0\n",
      "1997     ./skin-cancer-dataset/test/Benign   7075.jpg      0\n",
      "1998     ./skin-cancer-dataset/test/Benign   7061.jpg      0\n",
      "1999     ./skin-cancer-dataset/test/Benign   6419.jpg      0\n",
      "\n",
      "[2000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "tr_df['Class'].replace({'Benign': 0, 'Malignant': 1}, inplace=True)\n",
    "ts_df['Class'].replace({'Benign': 0, 'Malignant': 1}, inplace=True) \n",
    "print(tr_df, ts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferenciar los datos\n",
    "#### Partir las X Y\n",
    "Indicar al Dataframe train y test cuales son los datos y cuales son los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainX = tr_df.values[:, :-1]\n",
    "trainY = tr_df.values[:, -1]\n",
    "\n",
    "# Test\n",
    "testX = ts_df.values[:, :-1]\n",
    "testY = ts_df.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Dataset\n",
    "Read image te lo convierte en tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.image_path = X[:, 0]\n",
    "        self.image_name = X[:, 1]\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return read_image(str(Path(self.image_path[idx]) / self.image_name[idx])), self.Y[idx] # Lo devuelves directamente como una imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear el propi dataset\n",
    "Passar al train_dataloader i test_dataloader un objecte dataset, nosaltres hem de crear aquet dataset extenent de la clase Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = myDataset(trainX, trainY)\n",
    "test_dataset = myDataset(testX, testY)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear la red neuronal\n",
    "1. Crear el dispositivo\n",
    "2. Definir la clase *Module* con la función forward\n",
    "3. Crear el modelo y pasarlo a la GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Ya están implementadas las clases de las capas para hacer el forward\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 30, (5, 5)), # 30 imatges de (222x242)*3\n",
    "            nn.MaxPool2d((3, 3)), # 30 imatges de 74x80\n",
    "\n",
    "            nn.Conv2d(30, 120, (5, 5)), # 120 imatges de 72x78\n",
    "            nn.MaxPool2d((3, 3)), # 120 imatges de 14x16\n",
    "\n",
    "            nn.Conv2d(120, 250, (5, 5)), # 250 imatges de 12x14\n",
    "            nn.MaxPool2d((3, 3)), # 250 imatges de 4x4\n",
    "            \n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(250*4*4, 1000), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x) # Crea las capas\n",
    "        return logits\n",
    "    \n",
    "# Device\n",
    "#if torch.backends.mps.is_available():\n",
    "    #device = \"mps\"\n",
    "#else:\n",
    "device = \"cpu\"\n",
    "\n",
    "model = NeuralNetwork().to(device) # Otiene los valores predichos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train y test\n",
    "Definimos las funciones para train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "def train_loop(train_dataloader, model, loss_fn, optimizer):\n",
    "    size = len(train_dataloader.dataset)\n",
    "\n",
    "    for batch, (X, Y) in enumerate(train_dataloader):\n",
    "        X = X.to(device)\n",
    "        Y = nn.functional.one_hot(Y, num_classes=2) # One hot\n",
    "        Y = Y.to(device).to(int)\n",
    "        \n",
    "        pred = model(X) # Forward, ya ha calculado todos los gradientes\n",
    "        loss = loss_fn(pred, Y) # Crear la función de costo: error\n",
    "\n",
    "        loss.backward() # Le pasa el error al gradiente\n",
    "        optimizer.step() # Actualiza los valores\n",
    "        optimizer.zero_grad() # Pone el gradiente a 0\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(test_dataloader, model, loss_fn):\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # No calcula el gradiente automaticamente\n",
    "    with torch.no_grad():\n",
    "        for X, Y in test_dataloader:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            pred = model(X) # Forward\n",
    "            test_loss += loss_fn(pred, Y).item() # Error\n",
    "            correct += (pred.argmax(1) == Y).type(torch.float).sum().item() # Accuracy\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamar a train y test\n",
    "En cada epoca hacer un train y un test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_OpNamespace' 'image' object has no attribute 'read_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[362], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     train_loop(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m      9\u001b[0m     test_loop(test_dataloader, model, loss_fn)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[361], line 8\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(train_dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_loop\u001b[39m(train_dataloader, model, loss_fn, optimizer):\n\u001b[1;32m      6\u001b[0m     size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, Y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      9\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m         Y \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(Y, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# One hot\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[358], line 15\u001b[0m, in \u001b[0;36mmyDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_image(\u001b[38;5;28mstr\u001b[39m(Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_path[idx]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_name[idx])), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY[idx]\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torchvision/io/image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m--> 258\u001b[0m data \u001b[38;5;241m=\u001b[39m read_file(path)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torchvision/io/image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[0;32m---> 52\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mread_file(path)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/_ops.py:822\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m    820\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_get_operation(qualified_op_name)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    823\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m         )\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    829\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_OpNamespace' 'image' object has no attribute 'read_file'"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_OpNamespace' 'image' object has no attribute 'read_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[353], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# read the png image \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m pic \u001b[38;5;241m=\u001b[39m read_image(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/cynthia/Developer/Git/Formación/Dev Git ITIC/Artificial Inteligence/Neural Network/Skin-cancer/skin-cancer-dataset/test/Benign/6299.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# convert this torch tensor to PIL image  \u001b[39;00m\n\u001b[1;32m     10\u001b[0m PIL_img \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mToPILImage()(pic) \n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torchvision/io/image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m--> 258\u001b[0m data \u001b[38;5;241m=\u001b[39m read_file(path)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torchvision/io/image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[0;32m---> 52\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mread_file(path)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/_ops.py:822\u001b[0m, in \u001b[0;36m_OpNamespace.__getattr__\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m    820\u001b[0m     op, overload_names \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_get_operation(qualified_op_name)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    823\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m         )\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;66;03m# Turn this into AttributeError so getattr(obj, key, default)\u001b[39;00m\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;66;03m# works (this is called by TorchScript with __origin__)\u001b[39;00m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    829\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_OpNamespace\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_OpNamespace' 'image' object has no attribute 'read_file'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "from torchvision.io import read_image \n",
    "import torchvision.transforms as T \n",
    "\n",
    "# read the png image \n",
    "pic = read_image('/Users/cynthia/Developer/Git/Formación/Dev Git ITIC/Artificial Inteligence/Neural Network/Skin-cancer/skin-cancer-dataset/test/Benign/6299.jpg') \n",
    "  \n",
    "# convert this torch tensor to PIL image  \n",
    "PIL_img = T.ToPILImage()(pic) \n",
    "  \n",
    "# display result \n",
    "PIL_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
