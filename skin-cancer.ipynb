{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A modificar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotació de les imatges: url/label/rotación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el dataset\n",
    "1. Cargar el dataset que son imagenes que están dentro de carpetas separadas por test y train y cada una separada por maligno y benigno.\n",
    "2. Crearemos un dataset en el que por cada fila se incluye el path y si es benigno o maligno.\n",
    "3. Tenemos 4000 imagenes para el train y 2000 para el test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe train\n",
    "Obtener todas las imágenes dentro de la carpeta de train y guardar los Path junto con los nombres de las imagenes y si es maligna o benigna en un Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Class Path Image name      Class\n",
      "0     ./skin-cancer-dataset/train/Malignant     63.jpg  Malignant\n",
      "1     ./skin-cancer-dataset/train/Malignant    823.jpg  Malignant\n",
      "2     ./skin-cancer-dataset/train/Malignant   1409.jpg  Malignant\n",
      "3     ./skin-cancer-dataset/train/Malignant    189.jpg  Malignant\n",
      "4     ./skin-cancer-dataset/train/Malignant     77.jpg  Malignant\n",
      "...                                     ...        ...        ...\n",
      "3994     ./skin-cancer-dataset/train/Benign    190.jpg     Benign\n",
      "3995     ./skin-cancer-dataset/train/Benign   1404.jpg     Benign\n",
      "3996     ./skin-cancer-dataset/train/Benign   1410.jpg     Benign\n",
      "3997     ./skin-cancer-dataset/train/Benign    184.jpg     Benign\n",
      "3998     ./skin-cancer-dataset/train/Benign   1376.jpg     Benign\n",
      "\n",
      "[3999 rows x 3 columns]\n",
      "           Class Path  Image name\n",
      "Class                            \n",
      "Benign           2000        2000\n",
      "Malignant        1999        1999\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Train\n",
    "def train_df(train_path):\n",
    "    classes = [] # Benignas o malignas\n",
    "    class_paths = [] # Paths de las imagenes\n",
    "    image_names = [] # Nombre de las imagenes\n",
    "\n",
    "    # Obtener el contenido de la carpeta Train (Benigna y Maligna)\n",
    "    files = os.listdir(train_path)\n",
    "    for file in files: \n",
    "        label_dir = os.path.join(train_path, file)\n",
    "        label = os.listdir(label_dir)\n",
    "\n",
    "        # Obtener el contenido de la carpeta Benigna y Maligna (Imagenes)\n",
    "        for image in label:\n",
    "            if not image.startswith('.'): # Descartar ficheros ocultos\n",
    "                image_names.append(image) \n",
    "                class_paths.append(label_dir) \n",
    "                classes.append(file) \n",
    "\n",
    "    # Crear el dataframe\n",
    "    class_paths = pd.Series(class_paths, name='Class Path')\n",
    "    image_names = pd.Series(image_names, name='Image name')\n",
    "    image_classes = pd.Series(classes, name='Class') \n",
    "    tr_df = pd.concat([class_paths, image_names, image_classes], axis=1)\n",
    "    \n",
    "    return tr_df\n",
    "\n",
    "tr_df = train_df('./skin-cancer-dataset/train')\n",
    "print(tr_df)\n",
    "groupped = tr_df.groupby([\"Class\"]).count()\n",
    "print(groupped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe test\n",
    "Obtener todas las imágenes dentro de la carpeta de test y guardar los Path junto con los nombres de las imagenes y si es maligna o benigna en un Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     ts_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([class_paths, image_names, image_classes], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ts_df\n\u001b[0;32m---> 26\u001b[0m ts_df \u001b[38;5;241m=\u001b[39m \u001b[43mtest_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./skin-cancer-dataset/test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ts_df))\n",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m, in \u001b[0;36mtest_df\u001b[0;34m(test_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m class_paths \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# Ruta a las imagenes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m image_names \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# Nombre de las imagenes\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(test_path) \u001b[38;5;66;03m# Archivos\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files: \n\u001b[1;32m      9\u001b[0m     label_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_path, file) \u001b[38;5;66;03m# Path del directorio\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "def test_df(test_path):\n",
    "    classes = [] # Benignas o malignas\n",
    "    class_paths = [] # Ruta a las imagenes\n",
    "    image_names = [] # Nombre de las imagenes\n",
    "\n",
    "    files = os.listdir(test_path) # Archivos\n",
    "    for file in files: \n",
    "        label_dir = os.path.join(test_path, file) # Path del directorio\n",
    "        label = os.listdir(label_dir) # Imagenes dentro del directorio\n",
    "        for image in label:\n",
    "            if not image.startswith('.'):\n",
    "                image_names.append(image) \n",
    "                class_paths.append(label_dir) \n",
    "                classes.append(file) # Añade el nombre del archivo\n",
    "\n",
    "    # Series de pandas\n",
    "    class_paths = pd.Series(class_paths, name='Class Path')\n",
    "    image_names = pd.Series(image_names, name='Image name')\n",
    "    image_classes = pd.Series(classes, name='Class') \n",
    "\n",
    "    # Crea el dataframe\n",
    "    ts_df = pd.concat([class_paths, image_names, image_classes], axis=1)\n",
    "    return ts_df\n",
    "\n",
    "ts_df = test_df('./skin-cancer-dataset/test')\n",
    "print(len(ts_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizar los datos entre 0 y 1\n",
    "Normalizar los datos de Benigno y Maligno de string a numeros (0 o 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Class Path Image name  Class\n",
      "0     ./skin-cancer-dataset/train/Malignant     63.jpg      1\n",
      "1     ./skin-cancer-dataset/train/Malignant    823.jpg      1\n",
      "2     ./skin-cancer-dataset/train/Malignant   1409.jpg      1\n",
      "3     ./skin-cancer-dataset/train/Malignant    189.jpg      1\n",
      "4     ./skin-cancer-dataset/train/Malignant     77.jpg      1\n",
      "...                                     ...        ...    ...\n",
      "3994     ./skin-cancer-dataset/train/Benign    190.jpg      0\n",
      "3995     ./skin-cancer-dataset/train/Benign   1404.jpg      0\n",
      "3996     ./skin-cancer-dataset/train/Benign   1410.jpg      0\n",
      "3997     ./skin-cancer-dataset/train/Benign    184.jpg      0\n",
      "3998     ./skin-cancer-dataset/train/Benign   1376.jpg      0\n",
      "\n",
      "[3999 rows x 3 columns]                                 Class Path Image name  Class\n",
      "0     ./skin-cancer-dataset/test/Malignant   6400.jpg      1\n",
      "1     ./skin-cancer-dataset/test/Malignant   6366.jpg      1\n",
      "2     ./skin-cancer-dataset/test/Malignant   6372.jpg      1\n",
      "3     ./skin-cancer-dataset/test/Malignant   6414.jpg      1\n",
      "4     ./skin-cancer-dataset/test/Malignant   5653.jpg      1\n",
      "...                                    ...        ...    ...\n",
      "1995     ./skin-cancer-dataset/test/Benign   6343.jpg      0\n",
      "1996     ./skin-cancer-dataset/test/Benign   6425.jpg      0\n",
      "1997     ./skin-cancer-dataset/test/Benign   7075.jpg      0\n",
      "1998     ./skin-cancer-dataset/test/Benign   7061.jpg      0\n",
      "1999     ./skin-cancer-dataset/test/Benign   6419.jpg      0\n",
      "\n",
      "[2000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "tr_df['Class'].replace({'Benign': 0, 'Malignant': 1}, inplace=True)\n",
    "ts_df['Class'].replace({'Benign': 0, 'Malignant': 1}, inplace=True) \n",
    "print(tr_df, ts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diferenciar los datos\n",
    "### Partir las X Y\n",
    "Indicar al Dataframe train y test cuales son los datos y cuales son los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainX = tr_df.values[:, :-1]\n",
    "trainY = tr_df.values[:, -1]\n",
    "\n",
    "# Test\n",
    "testX = ts_df.values[:, :-1]\n",
    "testY = ts_df.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir les Y en tensor\n",
    "Les X las convertimos despues de obtener la imagen del Path en el Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "trainY = torch.tensor(trainY.astype(int))\n",
    "testY = torch.tensor(testY.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase Dataset\n",
    "Al obtener el dataset devolverá la imagen como tensor, que será la X y los resultados Y.\n",
    "\n",
    "### **Normalizar las imagenes**:\n",
    "Las imágenes están contenidas dentro de X en forma de Path por un lado y el image name por otro. Para ello tenemos que convertir los paths a una imagen y normalizar los datos para que sean entre 0 y 1 en vez de 0 a 224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as transform\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.image_path = X[:, 0]\n",
    "        self.image_name = X[:, 1]\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.Y)*4\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        PIL_image = PIL.Image.open(str(Path(self.image_path[int(idx/4)]) / self.image_name[int(idx/4)]))\n",
    "        tensor_image = transform.to_tensor(PIL_image)/255.0 # Convertir a tensor y normalizar\n",
    "        return self.__rotate__(tensor_image, idx%4), self.Y[int(idx/4)] # Lo devuelves directamente como una imagen\n",
    "    \n",
    "    def __rotate__(self, PIL_image, quarter):\n",
    "        return transform.rotate(PIL_image, 90*quarter) # Girar imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear el propi dataset\n",
    "Passar al train_dataloader i test_dataloader un objecte dataset, nosaltres hem de crear aquet dataset extenent de la clase Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = myDataset(trainX, trainY)\n",
    "test_dataset = myDataset(testX, testY)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear la red neuronal\n",
    "1. Crear el dispositivo\n",
    "2. Definir la clase *Module* con la función forward\n",
    "    - Convulacional (como es en color, cada imagen se multiplica x 3)\n",
    "    - Capas normales \n",
    "3. Crear el modelo y pasarlo a la GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Ya están implementadas las clases de las capas para hacer el forward\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # 3 imatges de 224 x 224\n",
    "            nn.Conv2d(3, 96, (11, 11), stride=4), # Entrada, salida, filtro, filtro cada... 53x53x96 mida - filtre /stride\n",
    "            nn.MaxPool2d((3, 3)), # 17x17x96\n",
    "\n",
    "            nn.Conv2d(96, 256, (3, 3), padding=4), #15x15x256\n",
    "            nn.MaxPool2d((3, 3), stride=2), # 5x5x256\n",
    "\n",
    "            nn.Conv2d(256, 256, (2, 2)), # \n",
    "            nn.MaxPool2d((3, 3), stride=2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Dropout2d(p=0.5, inplace=True), # Para que no haya sobreentrenamiento\n",
    "\n",
    "            nn.Linear(6400, 3000), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3000, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x) # Crea las capas\n",
    "        return logits\n",
    "    \n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "model = NeuralNetwork().to(device) # Otiene los valores predichos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train y test\n",
    "Definimos las funciones para train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "def train_loop(train_dataloader, model, loss_fn, optimizer):\n",
    "    size = len(train_dataloader.dataset)\n",
    "\n",
    "    for batch, (X, Y) in enumerate(train_dataloader):\n",
    "        X = X.to(device) \n",
    "        Y = Y.to(device) \n",
    "        \n",
    "        pred = model(X) # Forward, ya ha calculado todos los gradientes\n",
    "        loss = loss_fn(pred, Y) # Crear la función de costo: error\n",
    "\n",
    "        loss.backward() # Le pasa el error al gradiente\n",
    "        optimizer.step() # Actualiza los valores\n",
    "        optimizer.zero_grad() # Pone el gradiente a 0\n",
    "\n",
    "        # Accuracy\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(test_dataloader, model, loss_fn):\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    all_preds = torch.tensor([]).to(device)\n",
    "    \n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # No calcula el gradiente automaticamente\n",
    "    with torch.no_grad():\n",
    "        for X, Y in test_dataloader:\n",
    "            \n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            pred = model(X) # Forward\n",
    "            test_loss += loss_fn(pred, Y).item() # Error\n",
    "            correct += (pred.argmax(1) == Y).type(torch.float).sum().item() # Accuracy\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamar a train y test\n",
    "En cada epoca hacer un train y un test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/functional.py:1347: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/l4/zsmfmzjn78l0w5yh426c9cn00000gn/T/ipykernel_2831/111343696.py\", line 9, in <module>\n",
      "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
      "  File \"/var/folders/l4/zsmfmzjn78l0w5yh426c9cn00000gn/T/ipykernel_2831/377430500.py\", line 13, in train_loop\n",
      "    pred = model(X) # Forward, ya ha calculado todos los gradientes\n",
      "           ^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    tracing_state.pop_scope()\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    def _call_impl(self, *args, **kwargs):\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/l4/zsmfmzjn78l0w5yh426c9cn00000gn/T/ipykernel_2831/3050087176.py\", line 32, in forward\n",
      "    logits = self.linear_relu_stack(x) # Crea las capas\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    tracing_state.pop_scope()\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    def _call_impl(self, *args, **kwargs):\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    tracing_state.pop_scope()\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    def _call_impl(self, *args, **kwargs):\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 116, in forward\n",
      "    def extra_repr(self) -> str:\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: linear(): input and weight.T shapes cannot be multiplied (64x4096 and 6400x3000)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/Users/cynthia/anaconda3/envs/PyTorch/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = NeuralNetwork().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
